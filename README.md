# Kites

# Proyecto: Pipeline de Ingesta, Limpieza y Escritura de Datos de Clientes

Este pipeline orquesta la ingesta, transformaci√≥n y persistencia de datos de clientes desde un archivo `.csv` alojado en un repositorio p√∫blico, procesado en Databricks, y almacenado finalmente en Azure Data Lake.

---

## üß© Componentes del Pipeline: `pl_copy_git_to_datalake`

### 1. **Actividad: Copy to DataLake**
- **Tipo:** Copy
- **Origen:** Archivo CSV desde un repositorio HTTP (`dataset_git`)
- **Destino:** Azure Data Lake Gen2 (`dataset_datalake`)
- **Formato:** Delimited Text (`.txt`), con `quoteAllText = true`
- **M√©todo HTTP:** GET
- **Uso:** Descarga e ingesta el archivo crudo desde la fuente externa al Data Lake (zona *inbox*).

---

### 2. **Actividad: NotebookDatabricks**
- **Tipo:** DatabricksNotebook
- **Notebook Ejecutado:** `/Users/oreschz@hotmail.com/KaitNotebook`
- **Dependencia:** Se ejecuta solo si `Copy to DataLake` fue exitoso.
- **Uso:** 
  - Limpieza y normalizaci√≥n de datos.
  - Conversi√≥n de tipos de datos.
  - Enriquecimiento con tabla de dimensiones (join con `DimGeography`).
  - Aplicaci√≥n de l√≥gica SCD  para sobreescribir la tabla `DimCustomer` v√≠a JDBC.

---

### 3. **Actividad: copyProcessedData**
- **Tipo:** Copy
- **Origen:** Datos procesados por Databricks (`ds_customer_inbox`)
- **Destino:** Zona procesada en el Data Lake (`ds_customer_processed`)
- **Formato:** Delimited Text (`.txt`)
- **Comportamiento:** `FlattenHierarchy` para almacenar en estructura plana.
- **Uso:** Exporta los resultados finales procesados por Databricks nuevamente al Data Lake.

---

## üõ†Ô∏è Tecnolog√≠as Usadas

- **Azure Data Factory (ADF)**: Orquestaci√≥n de flujos ETL.
- **Azure Data Lake Storage Gen2**: Almacenamiento estructurado.
- **Azure Databricks**: Transformaci√≥n y limpieza con PySpark.
- **Azure SQL Database**: Almac√©n relacional destino (tabla `DimCustomer`).
- **PySpark**: Procesamiento distribuido de datos en Databricks.
- **JDBC**: Comunicaci√≥n entre Spark y SQL Server para escritura final.

---

## ‚öôÔ∏è L√≥gica del Notebook `KaitNotebook`

- Carga y limpieza de `CustomerData.csv`.
- Remoci√≥n de caracteres corruptos y acentos.
- Est√°ndar de ciudades como `"m√ºnchen" ‚Üí "muenchen"` antes de limpieza.
- Conversi√≥n de tipos (`String`, `Date`, `Double`, `Integer`, `Boolean`).
- Join con `DimGeography` usando `City` y `CountryRegionCode`.
- Generaci√≥n de `df_final_to_upsert` sin duplicar `CustomerKey`.
- Escritura final a `DimCustomer` v√≠a JDBC (`overwrite` mode).

---

## üß™ Siguientes Pasos Sugeridos

1. **Automatizaci√≥n del Pipeline:**
   - **Triggers basados en eventos:** Configurar un trigger en Azure Data Factory que detecte la llegada de nuevos archivos al contenedor `inbox` (usando eventos de blob storage).
   - **Triggers programados:** Agendar ejecuciones recurrentes (ej. cada hora, diariamente) para mantener los datos sincronizados autom√°ticamente.

2. **Optimizaci√≥n de Escritura en DimCustomer (SCD Tipo 1):**
   - Reemplazar el `.mode("overwrite")` por una estrategia m√°s segura como `merge` o `upsert`, evitando la p√©rdida de registros hist√≥ricos y minimizando riesgos en ambientes productivos.
   - Implementar una comparaci√≥n de registros basada en claves naturales (`CustomerAlternateKey`, `EmailAddress`, etc.) y columnas cr√≠ticas para detectar cambios.

3. **Logging y Auditor√≠a:**
   - Incorporar mecanismos de logging (mediante tablas de auditor√≠a o Azure Monitor) para rastrear ejecuciones, errores y tiempos de carga.
   - Registrar el n√∫mero de filas insertadas, actualizadas y descartadas.

4. **Validaciones Autom√°ticas:**
   - A√±adir pasos posteriores que verifiquen que no hay valores nulos en claves primarias (`GeographyKey`, `CustomerKey`).
   - Detectar y alertar sobre registros sin match con `DimGeography` para correcci√≥n de calidad de datos.

5. **Escalabilidad y Modularizaci√≥n:**
   - Parametrizar el pipeline para facilitar su uso con otros datasets (reutilizar el mismo flujo para diferentes or√≠genes).
   - Separar la l√≥gica de limpieza, transformaci√≥n y escritura en notebooks independientes y reutilizables.

---